{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\nimport gc\nimport random\nimport time\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport pywt\nfrom math import ceil\nimport scipy as sp\nfrom scipy import signal\nfrom scipy import fftpack\nfrom scipy.interpolate import CubicSpline\nfrom scipy.interpolate import interp1d\n\nimport json\nfrom tqdm import tqdm\nimport glob\nimport re\nfrom typing import Iterable\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n!pip install torcheval --no-index --find-links=file:///kaggle/input/torcheval/torcheval/\nfrom torcheval.metrics import MulticlassAUPRC\nfrom torcheval.metrics import BinaryAUPRC\n#import pytorch_lightning as pl\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.cuda.amp import GradScaler\n\nfrom sklearn.model_selection import train_test_split, StratifiedGroupKFold\nfrom sklearn.metrics import accuracy_score, average_precision_score\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-07T23:38:42.381787Z","iopub.execute_input":"2023-06-07T23:38:42.382510Z","iopub.status.idle":"2023-06-07T23:39:04.257900Z","shell.execute_reply.started":"2023-06-07T23:38:42.382472Z","shell.execute_reply":"2023-06-07T23:39:04.256654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    datasets_folder = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/'\n    step_data_folder = '/kaggle/input/private-steprate/features/'\n    working_folder = '/kaggle/working/'\n    tdcs_features_ds_folder = '/kaggle/input/tdcs-steps/kaggle/working/features/'\n    data_names = ('tdcsfog', 'defog')\n    #saved_features_file = '.csv'\n    frequencies = (128, 100)\n    g = 9.806\n\n    batch_size = 1\n    num_workers = 8\n    \n    feature_window_size = 128\n    \n    window_size = 512\n    window_front = 96\n    window_back = 96\n    window_body = window_size - window_front - window_back\n    #window_future = 8\n    #window_past = window_size - window_future\n    \n    model_dropout = 0.1\n    model_nlayers = 4\n    model_middles = [16, 12, 8, 4]\n    model_nblocks = 4\n    model_layers_grow = 0\n    \n    #lr = 0.00015\n    lr = 0.00002\n    num_epochs_per_turn = 20\n    num_turns = 8\n    eval_freq = 5\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    feature_list = ['acc_v', 'acc_ml', 'acc_ap']\n    nfeatures = len(feature_list)\n    label_list_full = ['start_hesitation', 'turn', 'walking']\n    label_list = label_list_full\n    #label_list = ['walking']\n    #label_list = ['start_hesitation']\n    #label_list = ['start_hesitation', 'walking']\n    nlabels = len(label_list)\n    \n    load_model_from_file = True\n    #model_file = '/kaggle/input/best-model-state-05/best_model_state_05.h5'\n    model_file = '/kaggle/input/model-state-03/model_state_02.h5'\n    \ncfg = Config()\n#cfg.lr = 0.00003\n#cfg.model_dropout = 0.1\n#cfg.num_epochs = 3","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.260397Z","iopub.execute_input":"2023-06-07T23:39:04.261592Z","iopub.status.idle":"2023-06-07T23:39:04.307094Z","shell.execute_reply.started":"2023-06-07T23:39:04.261555Z","shell.execute_reply":"2023-06-07T23:39:04.305935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.device","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.308539Z","iopub.execute_input":"2023-06-07T23:39:04.309435Z","iopub.status.idle":"2023-06-07T23:39:04.324424Z","shell.execute_reply.started":"2023-06-07T23:39:04.309280Z","shell.execute_reply":"2023-06-07T23:39:04.322555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_layers( nl = cfg.model_nlayers, grow = cfg.model_layers_grow ):\n    base = (cfg.window_front + cfg.window_back - grow * nl * (nl+1)//2)//nl\n    layers_list = np.array([1 + base + (i+1)*grow for i in range(nl)])\n    layers_list = layers_list + 1 - np.divmod(layers_list, 2)[1]\n    layers_list[-1] = cfg.window_front + cfg.window_back + nl - sum(layers_list[0:-1])\n    print('layers', layers_list)\n    print('zero is correct', sum(layers_list)- nl - cfg.window_front - cfg.window_back)\n    return layers_list\ncfg.layers = make_layers()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.327742Z","iopub.execute_input":"2023-06-07T23:39:04.328214Z","iopub.status.idle":"2023-06-07T23:39:04.339010Z","shell.execute_reply.started":"2023-06-07T23:39:04.328177Z","shell.execute_reply":"2023-06-07T23:39:04.337789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def camel_to_snake(name):\n    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n\ndef read_data_file(test_id, subfolder_name = 'train/tdcsfog/'):\n    try:\n        data = pd.read_csv(cfg.datasets_folder+subfolder_name + str(test_id) + '.csv')\n    except:\n        return pd.NA\n    data.columns = [camel_to_snake(c) for c in data.columns]\n    return data\n\ndef _find_binary(items: Iterable[int], item: int):\n    min = 0\n    max = len(items)-1\n    if items[max]<=item:\n        return max\n    while min<max:\n        split = (max+min)//2\n        item_split = items[split]\n        if item < item_split:\n            max = split\n        else:\n            min = split+1\n    return min","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.341037Z","iopub.execute_input":"2023-06-07T23:39:04.341524Z","iopub.status.idle":"2023-06-07T23:39:04.355388Z","shell.execute_reply.started":"2023-06-07T23:39:04.341489Z","shell.execute_reply":"2023-06-07T23:39:04.354279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_steps(signal, frequency=128, smoothing=2, faster_please=False, min_scale_log=3.6, max_scale_log=5.4):\n    \n    # scales for wavelets that define frequencies, exp for more even distribution\n    # choosing border values is tricky\n    # too narrow and we lose some slow or fast steprates\n    # too wide and we get the steprate wrong for unusual walking wavelet spectrum cases\n    # check patient #6 in the tdcsfog dataset as an example of an unusual pattern\n    scales = np.exp(np.arange(min_scale_log, max_scale_log, 0.05))\n    wavelet='morl'  # chosing the Morlet wavelet\n    \n    # transforming the signal, preferrably the 'AccMl' one\n    coeff, freq = pywt.cwt(signal, scales, wavelet)\n    # finding the brightest dots on every time slice\n    coeff_argmax_index = np.argmax(abs(coeff), 0)\n    # and collecting their coeff values in a new \"signal\", which already looks like our step rate, but is a bit noisy\n    coeff_max = np.array([coeff[coeff_argmax_index[i], i] for i in range(coeff.shape[1])])\n    \n    # repeating the smoothing procedure\n    for i in range(smoothing):\n        # transforming the noisy step rate again to clear it\n        coeff, freq = pywt.cwt(coeff_max, scales, wavelet)\n        # finding the brightest dots on every time slice\n        coeff_argmax_index = np.argmax(abs(coeff), 0).astype(int)\n        # and smoothing their indices with median to cut some outbursts off\n        coeff_argmax_index = np.round(pd.Series(coeff_argmax_index).rolling(128, center=True, min_periods=1).median()).astype(int)\n        # collecting the values along our line of indices\n        coeff_max = np.array([coeff[coeff_argmax_index[i], i] for i in range(coeff.shape[1])])\n    \n    # this smoothing round is optional\n    if not faster_please:\n        # finding hopefully bigger peaks on our line\n        # hopefully skiping too narrow or too close ones that are likely to be outbursts\n        max_cwt_points = sp.signal.find_peaks(abs(coeff_max), distance=20, width=20)[0]\n        max_cwt_points = np.concatenate(([0], max_cwt_points, [len(signal)-1]))\n        # and interpolating our line in wavelet space between the bigger peaks\n        max_cwt_line_indexes = np.round(np.interp( range(0, len(signal)), max_cwt_points, \n                                                    coeff_argmax_index[max_cwt_points])).astype(int)\n        coeff_max = np.array([coeff[max_cwt_line_indexes[i], i] for i in range(coeff.shape[1])])\n    \n    # finding zeroes on this smooth line to separate individual steps from other steps    \n    zero_crossings = np.where( np.diff(np.sign(pd.Series(coeff_max).rolling(10, center=True, min_periods=1).mean())))[0]\n    zero_crossings = np.concatenate( ([0], zero_crossings, [len(signal)]))    \n    #filling each step with its duration while cutting possible outbursts with median\n    step_lengths = []\n    for i in range(1, len(zero_crossings)):\n        step_lengths = np.concatenate( (step_lengths, [zero_crossings[i] - zero_crossings[i-1]]*(zero_crossings[i] - zero_crossings[i-1]) ))\n    step_durations = pd.Series(step_lengths).rolling(32, center=True, min_periods=1).median()\n    # making a nice and smooth step_rate array\n    step_rate = pd.Series( 1./step_durations )*frequency\n    step_rate = step_rate.where(step_rate<5, 0).rolling(frequency, center=True, min_periods=1).mean()\n    \n    return step_rate, step_durations/frequency, zero_crossings","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.357177Z","iopub.execute_input":"2023-06-07T23:39:04.357591Z","iopub.status.idle":"2023-06-07T23:39:04.376984Z","shell.execute_reply.started":"2023-06-07T23:39:04.357546Z","shell.execute_reply":"2023-06-07T23:39:04.375997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#folder_path_tdcs = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/'\n# folder_path_defog = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/'\n#all_files_tdcs = [file for file in os.listdir(folder_path_tdcs)]\n# all_files_defog = [file for file in os.listdir(folder_path_defog)]\n\n#df_tdcsfog = pd.DataFrame(all_files_tdcs, columns = ['id'])\n#df_tdcsfog['id'] = df_tdcsfog['id'].apply(lambda x: x.rsplit('.',1)[0])\n#df_tdcsfog['data'] = df_tdcsfog['id'].apply(read_data_file)\n\n#df_tdcsfog['fog'] = df_tdcsfog['data'].apply(lambda x: x[['walking', 'start_hesitation']].max().max())\n#train_ids, valid_ids = train_test_split(df_tdcsfog['id'], random_state=42, stratify = df_tdcsfog['fog'])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.379099Z","iopub.execute_input":"2023-06-07T23:39:04.379544Z","iopub.status.idle":"2023-06-07T23:39:04.393994Z","shell.execute_reply.started":"2023-06-07T23:39:04.379505Z","shell.execute_reply":"2023-06-07T23:39:04.392971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.mkdir('/features/')\n#os.mkdir('/features/zero_crossings/')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.395338Z","iopub.execute_input":"2023-06-07T23:39:04.397260Z","iopub.status.idle":"2023-06-07T23:39:04.408400Z","shell.execute_reply.started":"2023-06-07T23:39:04.397232Z","shell.execute_reply":"2023-06-07T23:39:04.407460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.random.rand(10)\na","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.410246Z","iopub.execute_input":"2023-06-07T23:39:04.410681Z","iopub.status.idle":"2023-06-07T23:39:04.424791Z","shell.execute_reply.started":"2023-06-07T23:39:04.410645Z","shell.execute_reply":"2023-06-07T23:39:04.423685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.random.rand(10)\nb = range(10)\nf = CubicSpline(b,a)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.431997Z","iopub.execute_input":"2023-06-07T23:39:04.432336Z","iopub.status.idle":"2023-06-07T23:39:04.444127Z","shell.execute_reply.started":"2023-06-07T23:39:04.432311Z","shell.execute_reply":"2023-06-07T23:39:04.442903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.working_folder = '/kaggle/working/'\ndef make_feature_file(data_row):\n    #data = data_row['data']\n    step_rate, step_durations, zero_crossings = detect_steps(data_row['data']['acc_ml'])\n    data_row['data']['step_rate'] = step_rate\n    data_row['data']['step_durations'] = step_durations\n    data_row['data'].to_csv(cfg.working_folder + 'features/'+data_row['id']+'.csv')\n    pd.DataFrame(zero_crossings).to_csv(cfg.working_folder + 'features/zero_crossings/'+data_row['id']+'.csv')\n    \n\n#def make_feature_files(dataset):\n#    for data_row in tqdm(dataset):\n#        data = data_row['data']\n#        step_rate, step_durations, zero_crossings = detect_steps(data['acc_ml'])\n#        data_row['data']['steprate'] = step_rate\n#        data_row['data']['step_durations'] = step_durations\n#        data_row['data']['zero_crossings'] = zero_crossings\n#        data_row['data'].to_csv('features/'+data_row['id']+'.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.446495Z","iopub.execute_input":"2023-06-07T23:39:04.446879Z","iopub.status.idle":"2023-06-07T23:39:04.455932Z","shell.execute_reply.started":"2023-06-07T23:39:04.446846Z","shell.execute_reply":"2023-06-07T23:39:04.454807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.457686Z","iopub.execute_input":"2023-06-07T23:39:04.458085Z","iopub.status.idle":"2023-06-07T23:39:04.465983Z","shell.execute_reply.started":"2023-06-07T23:39:04.458034Z","shell.execute_reply":"2023-06-07T23:39:04.465378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def download_model(model_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{model_name}.zip\"\n    command = f\"zip {zip_name} {model_name}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{model_name}.zip'))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.467342Z","iopub.execute_input":"2023-06-07T23:39:04.468360Z","iopub.status.idle":"2023-06-07T23:39:04.481681Z","shell.execute_reply.started":"2023-06-07T23:39:04.468307Z","shell.execute_reply":"2023-06-07T23:39:04.480679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#download_file('/kaggle/working/features/', 'features.zip')\ndownload_model('model_state_02.h5')\n#download_file('/kaggle/working/features/', 'features.zip')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.483115Z","iopub.execute_input":"2023-06-07T23:39:04.483531Z","iopub.status.idle":"2023-06-07T23:39:04.511019Z","shell.execute_reply.started":"2023-06-07T23:39:04.483500Z","shell.execute_reply":"2023-06-07T23:39:04.509975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#make_feature_files(df_tdcsfog)\n#try:\n#    os.mkdir(cfg.working_folder + 'features/')\n#    os.mkdir(cfg.working_folder + 'features/zero_crossings/')\n#except: \n#    pass\n\n#df_tdcsfog.apply(make_feature_file, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.512596Z","iopub.execute_input":"2023-06-07T23:39:04.512930Z","iopub.status.idle":"2023-06-07T23:39:04.517410Z","shell.execute_reply.started":"2023-06-07T23:39:04.512899Z","shell.execute_reply":"2023-06-07T23:39:04.516426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.wavelet_scales = np.exp(np.arange(2, 6, 0.5))\ncfg.wavelet_list = [ f'wave_{scale}' for scale in cfg.wavelet_scales ] \ncfg.wavelet='morl'\n#wavelet='shan'\n\ndef make_features_train(data, defog=False):\n    mean_center = True\n    if not defog:\n        data[['acc_v', 'acc_ap','acc_ml']] /= 9.806\n    rolling_window_size = cfg.feature_window_size\n    data['acc_ml_abs_rolling_mean'] = abs(data['acc_ml']).rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['acc_ml_abs_rolling_min'] = abs(data['acc_ml']).rolling(rolling_window_size, center=mean_center, min_periods=1).min()\n    data['acc_ml_abs_rolling_max'] = abs(data['acc_ml']).rolling(rolling_window_size, center=mean_center, min_periods=1).max()\n    data['acc_apv'] = (data['acc_ap']**2 + data['acc_v']**2)**0.5 - 1\n    data['acc_apv_rolling_mean'] = abs(data['acc_apv']).rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['acc_apv_rolling_delta'] = abs(data['acc_apv']).rolling(rolling_window_size, center=mean_center, min_periods=1).max() -\\\n        abs(data['acc_apv']).rolling(rolling_window_size, center=mean_center, min_periods=1).min()\n    data['angle'] = data['acc_ap'] / data['acc_v'].rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['angle_rolling_mean'] = data['angle'].rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['acc_v'] = data['acc_v']/3\n    data['defog'] = 1 if defog else -1\n    \n    coeff, freq = pywt.cwt(data['acc_ml'], cfg.wavelet_scales, cfg.wavelet)\n    for i, scale in enumerate(cfg.wavelet_scales):\n        data[f'wave_{scale}'] = coeff[i] \n    \n    coeff_argmax = pd.Series(freq[np.argmax(abs(coeff), 0)]*cfg.frequencies[0])\\\n        .rolling(50, center=mean_center, min_periods=1).median().rolling(50, center=mean_center, min_periods=1).max()\n    data['coeff_argmax'] = coeff_argmax\n    \n    return data\n\n\ndef make_features(data, defog = False): \n    if defog:\n        step_rate, step_durations, zero_crossings = detect_steps(data['acc_ml'], frequency=128, min_scale_log=3.35, max_scale_log=5.2)\n    else:\n        step_rate, step_durations, zero_crossings = detect_steps(data['acc_ml'])\n    data['step_rate'] = step_rate\n    data['step_durations'] = step_durations\n    \n    if defog:\n        new_values = rescale_freq_2(data.values, old_freq = 100, new_freq = 128)\n        new_data = pd.DataFrame(new_values, columns = data.columns)\n        data = new_data\n    \n    data = make_features_train(data, defog=defog)\n    return data\n    \n\ndef make_features_row(data_row, df_name):\n    data = data_row['data']\n    mean_center = True\n    rolling_window_size = rolling_window_size = cfg.feature_window_size\n    data['acc_ml_abs_rolling_mean'] = abs(data['acc_ml']).rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['acc_ml_abs_rolling_min'] = abs(data['acc_ml']).rolling(rolling_window_size, center=mean_center, min_periods=1).min()\n    data['acc_ml_abs_rolling_max'] = abs(data['acc_ml']).rolling(rolling_window_size, center=mean_center, min_periods=1).max()\n    #data['steprate'] = abs(data['acc_ml']).rolling(128, center=True, min_periods=1).max()\n    data['acc_apv'] = (data['acc_ap']**2 + data['acc_v']**2)**0.5 - 9.806\n    data['acc_apv_rolling_mean'] = abs(data['acc_apv']).rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['acc_apv_rolling_delta'] = abs(data['acc_apv']).rolling(rolling_window_size, center=mean_center, min_periods=1).max() -\\\n        abs(data['acc_apv']).rolling(rolling_window_size, center=mean_center, min_periods=1).min()\n    data['angle'] = data['acc_ap'] / data['acc_v'].rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['angle_rolling_mean'] = data['angle'].rolling(rolling_window_size, center=mean_center, min_periods=1).mean()\n    data['acc_v'] = data['acc_v']/3\n    #data['acc_ap'] = data['acc_ap']\n    tmp_df = pd.read_csv(cfg.tdcs_features_ds_folder + data_row['id'] + '.csv')\n    data = data.join(tmp_df[['steprate', 'step_durations']])\n    \n    scales = np.exp(np.arange(2, 6, 0.5))\n    wavelet='morl'\n    #wavelet='shan'\n    coeff, freq = pywt.cwt(data['acc_ml'], scales, wavelet)\n    cfg.wavelet_list = []\n    for i, scale in enumerate(scales):\n        data[f'wave_{scales}'] = coeff[i]\n        cfg.wavelet_list.append(f'wave_{scales}')\n    coeff_argmax = pd.Series(freq[np.argmax(abs(coeff), 0)]*cfg.frequencies[0])\\\n        .rolling(50, center=mean_center, min_periods=1).median().rolling(50, center=mean_center, min_periods=1).max()\n    data['coeff_argmax'] = coeff_argmax\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.519179Z","iopub.execute_input":"2023-06-07T23:39:04.519875Z","iopub.status.idle":"2023-06-07T23:39:04.548235Z","shell.execute_reply.started":"2023-06-07T23:39:04.519825Z","shell.execute_reply":"2023-06-07T23:39:04.547092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#df_tdcsfog['data'] = df_tdcsfog.apply(make_features_row, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.549918Z","iopub.execute_input":"2023-06-07T23:39:04.550639Z","iopub.status.idle":"2023-06-07T23:39:04.565674Z","shell.execute_reply.started":"2023-06-07T23:39:04.550601Z","shell.execute_reply":"2023-06-07T23:39:04.564575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.feature_list = [\n    #'acc_v',           doesn't help\n    'acc_ml', \n    #'acc_ap',\n    'acc_ml_abs_rolling_mean',\n    'acc_ml_abs_rolling_min',\n    'acc_ml_abs_rolling_max',\n    'acc_apv',\n    'acc_apv_rolling_mean',\n    'acc_apv_rolling_delta',\n    'defog',\n    #'angle',\n    'angle_rolling_mean',\n    'step_rate',\n #   'step_durations', doesn't help - maybe they were just too big\n #   'coeff_argmax'    doesn't help\n] + cfg.wavelet_list\n\ncfg.nfeatures = len(cfg.feature_list)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.568224Z","iopub.execute_input":"2023-06-07T23:39:04.568642Z","iopub.status.idle":"2023-06-07T23:39:04.575358Z","shell.execute_reply.started":"2023-06-07T23:39:04.568606Z","shell.execute_reply":"2023-06-07T23:39:04.574362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def len_rescale(length, old_freq = 100, new_freq = 128):\n    old_dt, new_dt = 1.0/old_freq, 1.0/new_freq\n    return len( np.arange(0, len(data_array)*old_dt, new_dt) )\n\ndef len_rescale_de(length):\n    return en_rescale(length, old_freq = 128, new_freq = 100)\n\ndef rescale_freq(data_array, old_freq = 100, new_freq = 128):\n    old_dt, new_dt = 1.0/old_freq, 1.0/new_freq\n    time_old = np.arange(0, len(data_array)*old_dt, old_dt)\n    time_new = np.arange(0, len(data_array)*old_dt, new_dt)\n    f = CubicSpline(time_old, data_array)\n    return np.array([f(t) for t in time_new])\n\ndef rescale_freq_2(data_array, old_freq = 100, new_freq = 128):\n    old_dt, new_dt = 1.0/old_freq, 1.0/new_freq\n    max_time = len(data_array)*old_dt - old_dt\n    time_old = np.linspace(0, max_time,len(data_array))\n    time_new = np.linspace(0, max_time,round(len(data_array)*new_freq/old_freq))\n    #print( len(time_old), data_array.shape )\n    f = interp1d(time_old, data_array, axis=0, kind = 'linear')\n    return f(time_new) #time_new.apply(f) #[f(t) for t in time_new]\n\ndef rescale_freq_de(data_array):\n    return rescale_freq_2(data_array, old_freq = 128, new_freq = 100)\n\ndef plus_minus_from_t(t):\n    diff = np.diff(t,1)\n    plus = np.where(diff==1)[0]\n    minus = np.where(diff==-1)[0]\n    if t[0]==1:\n        plus = np.concatenate( ([-1], plus))\n    if t[-1]==1:\n        minus = np.concatenate( (minus, [len(t)-1]))\n    return plus+1, minus+1","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.577332Z","iopub.execute_input":"2023-06-07T23:39:04.578201Z","iopub.status.idle":"2023-06-07T23:39:04.592812Z","shell.execute_reply.started":"2023-06-07T23:39:04.578103Z","shell.execute_reply":"2023-06-07T23:39:04.592113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cDataHolder():\n    def __init__(self):\n        return\n    \n    def make_fog_class(self, row, defog_split=4000, tdcs_split=2000):\n        if ( not row['tdcs'] ) and ( row[['fog_start_hesitation', 'fog_walking', 'fog_turn']].sum()>defog_split ):\n            fog_class = 'defog_yes'\n        elif ( not row['tdcs'] ): \n            fog_class = 'defog_no'\n        elif ( row['fog_walking'] == 0 ) and ( row['fog_start_hesitation'] == 0 ):\n            fog_class = 'fog_no_complex'\n        elif ( row['fog_walking'] == 0 ) and ( row['fog_start_hesitation'] < tdcs_split ):\n            fog_class = 'fog_some_hesitation'\n        elif ( row['fog_walking'] < tdcs_split) and ( row['fog_start_hesitation'] == 0 ):\n            fog_class = 'fog_some_walking'\n        elif ( row['fog_walking'] >= tdcs_split):\n            fog_class = 'fog_bad_walking'\n        elif ( row['fog_start_hesitation'] >= tdcs_split ):\n            fog_class = 'fog_bad_hesitation'\n        else:\n            fog_class = 'fog_some_complex'\n        return fog_class\n    \n   # def load_steps(self, data_id, ds_name):\n   #     ds_name = ds_name + '/'\n   #     file_df = pd.read_csv(cfg.datasets_folder + 'train/' + ds_name + data_id +'.csv')\n   #     file_df.columns = [camel_to_snake(c) for c in file_df.columns]\n   #     steps_df = pd.read_csv(cfg.step_data_folder + ds_name + data_id +'.csv')\n   #     file_df = file_df.join(steps_df)\n   #     return file_df\n    \n    def make_defog_task_list(self, file_number):\n        a = self.list_t[file_number]\n        diff = np.diff(a,1)\n        plus = np.where(diff==1)[0]\n        minus = np.where(diff==-1)[0]\n        return np.vstack( (plus+1, minus+1) )\n    \n    def process_defog_file(self, file_data, just_task=False):\n        t_big = (file_data['task']*file_data['valid']).values.astype(int)\n        if just_task:\n            t_big = (file_data['task']).values.astype(int)          \n        diff = np.diff(t_big,1)\n        plus = np.where(diff==1)[0]\n        minus = np.where(diff==-1)[0]\n        count = len(plus)\n        x = np.zeros( [(minus-plus).sum() + 2*cfg.window_front*(count), cfg.nfeatures] )\n        y = np.zeros( ((minus-plus).sum() + 2*cfg.window_front*(count - 1)) )\n        y3 = np.zeros( ((minus-plus).sum() + 2*cfg.window_front*(count - 1), 3) )\n        t = np.zeros( ((minus-plus).sum() + 2*cfg.window_front*(count - 1)) )\n        #print( len(t_big), (minus-plus).sum() , len(plus))\n        \n        index0, index1 = 0, 0\n        sum_ws = 0\n        for first, count in zip (plus, minus-plus):\n            #print(first, count, first+count)\n            wsize = count + 2*cfg.window_front\n            sum_ws += wsize\n            index0 = first - cfg.window_front\n            \n            index_fix = index0 if index0>=0 else 0\n            index_last_fix = index0 + wsize if index0 + wsize< len(file_data) else len(file_data)\n            length_fix = index0 - index_fix + index_last_fix - index0 - wsize\n            \n            features = file_data[cfg.feature_list]\n            if index1 < 0:\n                x[ 0 : index1 + wsize] = features.values[ index0 + index1: index0 + wsize ]\n                print('shit')\n            elif index0 + wsize>features.values.shape[0]:\n                print('fuck')\n                x[ index1 : index1 + features.values.shape[0].shape[0] - index0] = features.values[ index0: features.values.shape[0].shape[0] ]\n            else:\n                x[ index1 : index1 + wsize] = features.values[ index0: index0 + wsize ]\n                        \n            targets = file_data[cfg.label_list].values\n            targets = np.concatenate([targets, (1 - targets.sum(axis=1)).reshape(-1,1)], axis=1)\n            y[ index1: index1 + count] = np.argmax(targets, axis=1)[ \n                        index0 + cfg.window_front : index0 + count + cfg.window_front ]\n            y3[ index1: index1 + count] = file_data[cfg.label_list].values[ \n                        index0 + cfg.window_front : index0 + count + cfg.window_front ]\n            t[ index1 : index1 + count] = 1\n            index1 += wsize\n\n        return x, y, y3, t\n    \n    def prepare_data_row(self, data_id, ds_name):\n        ds_name = ds_name + '/'\n        file_df = pd.read_csv(cfg.datasets_folder + 'train/' + ds_name + data_id +'.csv')\n        file_df.columns = [camel_to_snake(c) for c in file_df.columns]\n        steps_df = pd.read_csv(cfg.step_data_folder + ds_name + data_id +'.csv')\n        file_df = file_df.join(steps_df)\n        \n        # ----------------- rescale time here\n        if ds_name == 'defog/':\n            new_df = pd.DataFrame()\n            time_old = np.arange(0, len(file_df)/100, 0.01)\n\n            new_df = pd.DataFrame( np.vstack( rescale_freq_2(file_df.values) ) )\n            new_df.columns = file_df.columns\n            file_df = new_df\n            file_df = make_features_train(file_df)\n            x, y, y3, t = self.process_defog_file(file_df)\n#            self.list_t.append( np.vstack( ((file_df['task']*file_df['valid']).values.astype(int),\n#                                                file_df['task'].astype(int).values,\n#                                                file_df['valid'].values.astype(int)) ) )\n            self.list_t.append(t)\n        else:\n            #self.list_t.append( np.ones( (3, len(file_df)) ) )            \n            self.list_t.append( np.ones( len(file_df) ) )\n            file_df = make_features_train(file_df)\n            x = file_df[cfg.feature_list].values\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n            targets = file_df[cfg.label_list].values\n            y = np.concatenate([targets, (1 - targets.sum(axis=1)).reshape(-1,1)], axis=1)\n            y = np.argmax(y, axis=1)\n            y3 = targets\n            #print(len(file_df), y.shape, y3.shape, x.shape)\n            \n                \n        self.list_x.append(x)     \n        self.list_y.append(y)\n        self.list_y3.append(y3)        \n        return len(self.list_x)-1\n    \n    def load_train_data(self, load_tdcs = True, load_defog = True):\n        self.list_x, self.list_y, self.list_t, self.list_y3, self.task_list = [], [], [], [], []\n        if load_tdcs:\n            folder_path_tdcs = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/'\n            all_files_tdcs = [file for file in os.listdir(folder_path_tdcs)]\n            self.df_tdcs = pd.DataFrame(all_files_tdcs, columns = ['id'])\n            self.df_tdcs['id'] = self.df_tdcs['id'].apply(lambda x: x.rsplit('.',1)[0])\n\n            for index, row in tqdm(self.df_tdcs.iterrows()):\n                self.prepare_data_row(row['id'], 'tdcsfog') \n                self.task_list.append([0, dataholder.list_x[index].shape[0]])\n\n            self.df_tdcs['tdcs'] = True\n            self.df = self.df_tdcs.copy()\n        \n        if load_defog:\n            folder_path_defog = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/'\n            all_files_defog = [file for file in os.listdir(folder_path_defog)]\n            #all_files_defog = all_files_defog[:10]  #------------------------------------------------------ DEBUG!!\n            self.df_defog = pd.DataFrame(all_files_defog, columns = ['id'])\n            self.df_defog['id'] = self.df_defog['id'].apply(lambda x: x.rsplit('.',1)[0])\n\n            for index, row in tqdm(self.df_defog.iterrows()):\n                self.prepare_data_row(row['id'], 'defog')\n                self.task_list.append( self.make_defog_task_list(index) )\n\n            self.df_defog['tdcs'] = False\n            if load_tdcs:\n                print(len(self.df), len(self.df_tdcs), len(self.df_defog))\n                self.df = self.df.append(self.df_defog).reset_index(drop=True)\n                print(len(self.df))\n            else:\n                self.df = self.df_defog\n                \n        for index, label in enumerate(cfg.label_list):\n            dataholder.df['fog_'+label] = dataholder.df.reset_index()['index'].apply( lambda x: dataholder.list_y3[x][:,index].sum())\n        self.df['fog_class'] = self.df.apply(self.make_fog_class, axis=1)\n        \n    #def reorganize_defog()\n    \n    def make_a_train_valid_combo(self, complex_mult=3, train_tdcs=True, train_defog=True, stratify=True, val_tdcs=True, val_defog=True):\n        all_nums = self.df.index\n        if stratify:   \n            train_nums, valid_nums = train_test_split(self.df.index, test_size=0.1, stratify = self.df['fog_class'])\n        else:\n            train_nums, valid_nums = train_test_split(self.df.index)\n        if not train_tdcs:\n            train_nums = self.df.loc[train_nums][~self.df['tdcs']].index\n        if not val_defog:\n            train_nums = self.df.loc[train_nums][self.df['tdcs']].index\n        if not val_tdcs:\n            valid_nums = self.df.loc[valid_nums][~self.df['tdcs']].index\n        if not val_defog:\n            valid_nums = self.df.loc[valid_nums][self.df['tdcs']].index\n            \n        df_train = self.df.loc[train_nums]['fog_class']\n        df_complex = df_train[ (df_train!='fog_no_complex') & (df_train!='defog_no') ]\n        df_fog = df_train[ (df_train=='fog_no_complex') | (df_train=='defog_no') ]\n        #print(len(df_fog), len(df_complex))\n        for i in range(complex_mult):\n            df_fog = df_fog.append(df_complex)            \n        train_nums = df_fog.sample(frac=1).index\n        return train_nums, valid_nums\n            \n        \n    def prepare_train_valid(self, complex_mult=3):    \n        self.train_nums, self.valid_nums = train_test_split(self.df.index, stratify = self.df['fog_class'])\n        df_train = self.df.loc[self.train_nums]['fog_class']\n        df_complex = df_train[ (df_train!='fog_no_complex') & (df_train!='defog_no') ]\n        df_fog = df_train[ (df_train=='fog_no_complex') | (df_train=='defog_no') ]\n        #print(len(df_fog), len(df_complex))\n        for i in range(complex_mult):\n            df_fog = df_fog.append(df_complex)            \n        self.train_nums = df_fog.sample(frac=1).index\n        \n        df_train = self.df.loc[self.df[self.df['tdcs']].index] ['fog_class']\n        df_complex = df_train[df_train!='fog_no_complex']\n        df_fog = df_train[df_train=='fog_no_complex']\n        #print(len(df_fog), len(df_complex))\n        for i in range(10):\n            df_fog = df_fog.append(df_complex)            \n        self.full_tdcs_batch = df_fog.sample(frac=1).index\n    \n    \n    def get_data(self, index, add_zeros = False):\n        x = self.list_x[index]\n        y = self.list_y[index]\n        t = self.list_t[index]\n        #t = self.task_list[index]\n        if add_zeros:\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n            \n        x = torch.tensor(x).to(torch.float32)\n        y = torch.Tensor(y).to(torch.int64)\n        t = torch.Tensor(t).to(torch.int64)\n        return x, y, t\n        \n    def __getitem__(self, index, is_train, add_zeros = False):\n        file_index = self.train_nums[index] if is_train else self.valid_nums[index]\n        return self.get_data(file_index, add_zeros)\n    \n    def get_file_batch(self, file_index, add_zeros = False, want_valid = False, want_fog = False, want_ugly = False, want_defog=False):\n        if want_defog and want_ugly:\n            file_index = self.df.loc[self.valid_nums].query('fog_start_hesitation*for_walking>0 and tdcs').index[file_index]\n        elif want_defog:\n            file_index = self.df.loc[self.valid_nums].query('tdcs').index[file_index]\n        if want_ugly:\n            file_index = self.df.loc[self.valid_nums].query('fog_start_hesitation*fog_walking>0').index[file_index]\n        elif want_fog:\n            file_index = self.df.loc[self.valid_nums].query('fog_start_hesitation+fog_walking+fog_turn>0').index[file_index]\n        elif want_valid:\n            file_index = self.df.loc[self.valid_nums].index[file_index]\n        return  self.get_data(file_index, add_zeros)\n    \n    def get_file_batch_id(self, file_id, add_zeros = False):\n        file_index = self.df[ self.df['id']==file_id ].index[0]\n        return self.get_data(file_index)\n    \n    def make_file_batch(self, file_data, add_zeros = False, with_targets=False):\n        x = file_data[cfg.feature_list].values\n        if add_zeros:\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n        x = torch.tensor(x).to(torch.float32).view(1, -1, cfg.nfeatures)\n        return x\n    \ndataholder = cDataHolder()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.594290Z","iopub.execute_input":"2023-06-07T23:39:04.595289Z","iopub.status.idle":"2023-06-07T23:39:04.873659Z","shell.execute_reply.started":"2023-06-07T23:39:04.595252Z","shell.execute_reply":"2023-06-07T23:39:04.872264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path_defog = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/'\nall_files_defog = [file for file in os.listdir(folder_path_defog)]\ndefog_file = pd.read_csv(folder_path_defog + all_files_defog[0])\ndefog_file.columns = [camel_to_snake(c) for c in defog_file.columns]\ndefog_file = make_features(defog_file, defog=True)\nx, y, y3, t = dataholder.process_defog_file(defog_file)\n#defog_file","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:04.875948Z","iopub.execute_input":"2023-06-07T23:39:04.877187Z","iopub.status.idle":"2023-06-07T23:39:11.243704Z","shell.execute_reply.started":"2023-06-07T23:39:04.877150Z","shell.execute_reply":"2023-06-07T23:39:11.242623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figs, axs = plt.subplots( 6,1, figsize=(18,10))\naxs[0].plot(t)\nt_big = (defog_file['task']*defog_file['valid']).values.astype(int)\naxs[1].plot(t_big)\naxs[2].plot((defog_file['task']).values.astype(int))\naxs[3].plot((defog_file['valid']).values.astype(int))\naxs[4].plot((defog_file[['acc_ap','acc_v','acc_ml']].sum(axis=1)).values.astype(int))\naxs[5].plot((defog_file['acc_ml'].values))\n#axs[2].plot(defog_file[cfg.feature_list[0]].values[:20000])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:11.245454Z","iopub.execute_input":"2023-06-07T23:39:11.246098Z","iopub.status.idle":"2023-06-07T23:39:12.503659Z","shell.execute_reply.started":"2023-06-07T23:39:11.246034Z","shell.execute_reply":"2023-06-07T23:39:12.502746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(t)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:12.504689Z","iopub.execute_input":"2023-06-07T23:39:12.505018Z","iopub.status.idle":"2023-06-07T23:39:12.512397Z","shell.execute_reply.started":"2023-06-07T23:39:12.504986Z","shell.execute_reply":"2023-06-07T23:39:12.511435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = defog_file[cfg.feature_list]\ntmp[:3].shape","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:12.514167Z","iopub.execute_input":"2023-06-07T23:39:12.514858Z","iopub.status.idle":"2023-06-07T23:39:12.534512Z","shell.execute_reply.started":"2023-06-07T23:39:12.514826Z","shell.execute_reply":"2023-06-07T23:39:12.533492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path_defog = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/'\nall_files_defog = [file for file in os.listdir(folder_path_defog)]\ndf_defog = pd.DataFrame(all_files_defog, columns = ['id'])\ntmp_defog = pd.read_csv(folder_path_defog+df_defog.loc[0,'id'])\ntmp_defog","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:12.535902Z","iopub.execute_input":"2023-06-07T23:39:12.536907Z","iopub.status.idle":"2023-06-07T23:39:12.698616Z","shell.execute_reply.started":"2023-06-07T23:39:12.536872Z","shell.execute_reply":"2023-06-07T23:39:12.697478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataholder.load_train_data(load_tdcs=True, load_defog=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:39:12.700249Z","iopub.execute_input":"2023-06-07T23:39:12.700700Z","iopub.status.idle":"2023-06-07T23:44:10.950191Z","shell.execute_reply.started":"2023-06-07T23:39:12.700650Z","shell.execute_reply":"2023-06-07T23:44:10.949199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#tmplist = []\n#for i in range(91):\n#    tmplist.append()\ndataholder.task_list[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:10.951805Z","iopub.execute_input":"2023-06-07T23:44:10.954779Z","iopub.status.idle":"2023-06-07T23:44:10.962396Z","shell.execute_reply.started":"2023-06-07T23:44:10.954737Z","shell.execute_reply":"2023-06-07T23:44:10.961241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataholder.df","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:10.973862Z","iopub.execute_input":"2023-06-07T23:44:10.974218Z","iopub.status.idle":"2023-06-07T23:44:10.992156Z","shell.execute_reply.started":"2023-06-07T23:44:10.974189Z","shell.execute_reply":"2023-06-07T23:44:10.991014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataholder.df['fog_class'].value_counts()\n#dataholder.df[~dataholder.df['tdcs']].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:10.994163Z","iopub.execute_input":"2023-06-07T23:44:10.995313Z","iopub.status.idle":"2023-06-07T23:44:11.013907Z","shell.execute_reply.started":"2023-06-07T23:44:10.995271Z","shell.execute_reply":"2023-06-07T23:44:11.012756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataholder.prepare_train_valid(complex_mult=1)\n#dataholder.train_nums = dataholder.full_tdcs_batch\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.015648Z","iopub.execute_input":"2023-06-07T23:44:11.017332Z","iopub.status.idle":"2023-06-07T23:44:11.039313Z","shell.execute_reply.started":"2023-06-07T23:44:11.017303Z","shell.execute_reply":"2023-06-07T23:44:11.038219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_data = dataholder.df.loc[dataholder.valid_nums]\nvalid_data","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.041034Z","iopub.execute_input":"2023-06-07T23:44:11.041836Z","iopub.status.idle":"2023-06-07T23:44:11.065323Z","shell.execute_reply.started":"2023-06-07T23:44:11.041796Z","shell.execute_reply":"2023-06-07T23:44:11.064150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmp = dataholder.get_file_batch(0, add_zeros = False, want_valid = False, want_fog = False, want_ugly = False, want_defog=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.066802Z","iopub.execute_input":"2023-06-07T23:44:11.067956Z","iopub.status.idle":"2023-06-07T23:44:11.073810Z","shell.execute_reply.started":"2023-06-07T23:44:11.067915Z","shell.execute_reply":"2023-06-07T23:44:11.072015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#full_tdcs_batch\n#len(dataholder.full_tdcs_batch)\n#df_train = dataholder.df.loc[dataholder.df[dataholder.df['tdcs']].index] ['fog_class']\n#len(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.075920Z","iopub.execute_input":"2023-06-07T23:44:11.076441Z","iopub.status.idle":"2023-06-07T23:44:11.088230Z","shell.execute_reply.started":"2023-06-07T23:44:11.076402Z","shell.execute_reply":"2023-06-07T23:44:11.087096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#self.train_nums, self.valid_nums = train_test_split(self.df.index, stratify = self.df['fog_class'])\n#        df_train = self.df.loc[self.train_nums]['fog_class']\n#        df_fog = df_train[df_train=='fog_no_complex']\n#        #print(len(df_fog), len(df_complex))\n#        for i in range(10):\n#            df_fog = df_fog.append(df_complex)            \n#       self.train_nums = df_fog.sample(frac=1).index\n#       \n#       df_train = self.df.loc[self.df[self.df['tdcs']].index] ['fog_class']\n#        df_complex = df_train[df_train!='fog_no_complex']\n#        df_fog = df_train[df_train=='fog_no_complex']\n#        #print(len(df_fog), len(df_complex))\n#        for i in range(10):\n#            df_fog = df_fog.append(df_complex)            \n#        self.full_tdcs_batch = df_fog.sample(frac=1).index","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.090350Z","iopub.execute_input":"2023-06-07T23:44:11.090948Z","iopub.status.idle":"2023-06-07T23:44:11.099843Z","shell.execute_reply.started":"2023-06-07T23:44:11.090909Z","shell.execute_reply":"2023-06-07T23:44:11.098793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for index, row in tqdm(dataholder.df.iterrows()):\n#            row['fog'] = dataholder.list_y[cfg.label_list.index('walking')].sum()\n#label_list[1]\n#for index, label in enumerate(cfg.label_list):\n#    dataholder.df['fog_'+label] = dataholder.df.reset_index()['index'].apply( lambda x: dataholder.list_y[x][:,index].sum())\n#dataholder.df['walking_fog'] = dataholder.df.reset_index()['index'].apply( lambda x: dataholder.list_y[x][:,cfg.label_list.index('turn')].sum())\n#dataholder.df.sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.101082Z","iopub.execute_input":"2023-06-07T23:44:11.101389Z","iopub.status.idle":"2023-06-07T23:44:11.114254Z","shell.execute_reply.started":"2023-06-07T23:44:11.101363Z","shell.execute_reply":"2023-06-07T23:44:11.112861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmp_df['fog_class'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.116114Z","iopub.execute_input":"2023-06-07T23:44:11.116467Z","iopub.status.idle":"2023-06-07T23:44:11.124650Z","shell.execute_reply.started":"2023-06-07T23:44:11.116438Z","shell.execute_reply":"2023-06-07T23:44:11.123285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmp_df.query( 'fog_start_hesitation>0 or fog_turn>0 or fog_walking>0')['fog_walking'].hist(bins=100, figsize=(18,3))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.125876Z","iopub.execute_input":"2023-06-07T23:44:11.126221Z","iopub.status.idle":"2023-06-07T23:44:11.135584Z","shell.execute_reply.started":"2023-06-07T23:44:11.126197Z","shell.execute_reply":"2023-06-07T23:44:11.134590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x, y ,t = ds_train.__getitem__(0)\n#print(x.shape, len(y), len(t))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.137366Z","iopub.execute_input":"2023-06-07T23:44:11.137682Z","iopub.status.idle":"2023-06-07T23:44:11.147410Z","shell.execute_reply.started":"2023-06-07T23:44:11.137646Z","shell.execute_reply":"2023-06-07T23:44:11.146265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cFOGDataset_for_dataholder(Dataset):\n    def __init__(self, is_train=True, split=\"train\"):\n        self.is_train = is_train\n        self.split = split\n        return\n    def __len__(self):\n        return len(dataholder.train_nums) if self.is_train else len(dataholder.valid_nums)\n    def __getitem__(self, index, add_zeros = False):\n        return( dataholder.__getitem__(index, self.is_train, add_zeros) )\n    \nclass cFOGDataset_for_dataholder_premade_split(Dataset):\n    def __init__(self, file_ids):\n        self.file_ids = file_ids\n        return\n    def __len__(self):\n        return len(self.file_ids)\n    def __getitem__(self, index):\n        return( dataholder.get_data(self.file_ids[index]) )\n    \nds_train = cFOGDataset_for_dataholder(is_train=True, split = 'train')\nds_valid = cFOGDataset_for_dataholder(is_train=False, split = 'valid')\nds_full = cFOGDataset_for_dataholder(is_train=True, split = 'full')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.148868Z","iopub.execute_input":"2023-06-07T23:44:11.149337Z","iopub.status.idle":"2023-06-07T23:44:11.161203Z","shell.execute_reply.started":"2023-06-07T23:44:11.149169Z","shell.execute_reply":"2023-06-07T23:44:11.160125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dataholder.df.loc[dataholder.train_nums][~dataholder.df['tdcs']]\ndataholder.df.loc[train.index]\n#dataholder.train_nums.min()\n#dataholder.df.index","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.162794Z","iopub.execute_input":"2023-06-07T23:44:11.163480Z","iopub.status.idle":"2023-06-07T23:44:11.193146Z","shell.execute_reply.started":"2023-06-07T23:44:11.163445Z","shell.execute_reply":"2023-06-07T23:44:11.191994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#complex_mult=3, train_tdcs=True, train_defog=True, stratify=True, val_tdcs=True, val_defog=True):\ndefog_train, defog_valid = dataholder.make_a_train_valid_combo(complex_mult=7, train_tdcs=False, val_tdcs=False)\ntdcs_train, tdcs_valid = dataholder.make_a_train_valid_combo(complex_mult=7, train_defog=False, val_defog=False)\nboth_train, both_valid = dataholder.make_a_train_valid_combo(complex_mult=7)\nprint( len(defog_train), len(defog_valid))\nprint( len(tdcs_train), len(tdcs_valid))\nprint( len(both_train), len(both_valid))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.194940Z","iopub.execute_input":"2023-06-07T23:44:11.195698Z","iopub.status.idle":"2023-06-07T23:44:11.230153Z","shell.execute_reply.started":"2023-06-07T23:44:11.195660Z","shell.execute_reply":"2023-06-07T23:44:11.229084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_defog_train = cFOGDataset_for_dataholder_premade_split(defog_train)\nds_defog_valid = cFOGDataset_for_dataholder_premade_split(defog_valid)\nds_tdcs_train = cFOGDataset_for_dataholder_premade_split(tdcs_train)\nds_tdcs_valid = cFOGDataset_for_dataholder_premade_split(tdcs_valid)\nds_both_train = cFOGDataset_for_dataholder_premade_split(both_train)\nds_both_valid = cFOGDataset_for_dataholder_premade_split(both_valid)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.231960Z","iopub.execute_input":"2023-06-07T23:44:11.232763Z","iopub.status.idle":"2023-06-07T23:44:11.240240Z","shell.execute_reply.started":"2023-06-07T23:44:11.232722Z","shell.execute_reply":"2023-06-07T23:44:11.239110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cFOGDataset_06(Dataset):\n    def __init__(self, init_data, is_train=True, split=\"train\"):\n        self.df = init_data.loc[:, init_data.columns != 'data']\n        self.df['t_count'] = init_data['data'].apply(lambda x: x['time'].count())\n        self.df['window_count'] = np.ceil(self.df['t_count'] / cfg.window_body ).astype(int)\n        self.df['window_sum'] = self.df['window_count'].cumsum()\n        self.df['window_extra'] = self.df['window_count']*cfg.window_body - self.df['t_count']\n        self.is_train = is_train\n        self.split = split\n        self.list_x, self.list_y = [], []\n        for i in range(len(init_data)):\n            self.list_x.append(init_data.loc[i, 'data'][cfg.feature_list].values)\n            if is_train:\n                targets = init_data.loc[i, 'data'][cfg.label_list].values\n                self.list_y.append( np.concatenate([targets, (1 - targets.sum(axis=1)).reshape(-1,1)], axis=1) )  \n        \n    def __len__from_05(self):\n        return self.df['window_count'].sum()\n    \n    def __len__(self):\n        return self.df['window_count'].count()\n    \n    def get_index_y(self, index):\n        file_index = _find_binary(self.df['window_sum'], index)\n        infile_index = index if file_index==0 else index - self.df['window_sum'][file_index-1]\n        time_index = infile_index * cfg.window_body\n        return file_index, infile_index, time_index\n    \n    def get_file_batch(self, file_index, add_zeros = True):\n        x = self.list_x[file_index]\n        y = self.list_y[file_index]\n        if add_zeros:\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n        x = torch.tensor(x).to(torch.float32).view(1, -1, cfg.nfeatures)\n        y = torch.Tensor(np.argmax(y,axis=1)).to(torch.int64).view(1, -1)\n        return x, y, self.df.loc[file_index, 'id']\n    \n    def get_file_batch_id(self, file_id, add_zeros = True):\n        file_index = self.df[ self.df['id']==file_id ].index[0]\n        x = self.list_x[file_index]\n        y = self.list_y[file_index]\n        if add_zeros:\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n        x = torch.tensor(x).to(torch.float32).view(1, -1, cfg.nfeatures)\n        y = torch.Tensor(np.argmax(y,axis=1)).to(torch.int64).view(1, -1)\n        return x, y\n    \n    def make_file_batch(self, file_data, add_zeros = True, with_targets=False):\n        x = file_data[cfg.feature_list].values\n        if add_zeros:\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n        x = torch.tensor(x).to(torch.float32).view(1, -1, cfg.nfeatures)\n        return x\n    \n    def __getitem__(self, index, verbose = False, add_zeros = True):\n        x = self.list_x[index]\n        y = self.list_y[index]\n        if add_zeros:\n            x = np.concatenate(( np.zeros((cfg.window_front, x.shape[1])) , x, np.zeros((cfg.window_front, x.shape[1])) ))\n        x = torch.tensor(x).to(torch.float32)#.view(1, -1, cfg.nfeatures)\n        y = torch.Tensor(np.argmax(y,axis=1)).to(torch.int64)#.view(-1, 1)\n        return x, y\n    \n    def __getitem__from_05(self, index, verbose = False, add_zeros = True):\n        file_index = _find_binary(self.df['window_sum'], index)\n        infile_index = index if file_index==0 else index - self.df['window_sum'][file_index-1]\n        #time_index = infile_index * cfg.window_body - cfg.window_front\n        time_index = infile_index * cfg.window_body\n        x_index = time_index - cfg.window_front\n        front_zeros = 0\n        if x_index < 0:\n            front_zeros = -x_index\n            x_index = 0\n        \n        x = self.list_x[file_index][ x_index : x_index + cfg.window_size - front_zeros ]\n        y = self.list_y[file_index][ time_index : time_index + cfg.window_body ]\n        \n        rear_zeros = cfg.window_size - x.shape[0] - front_zeros\n        \n        if front_zeros or rear_zeros:\n            if add_zeros and front_zeros:\n                x = np.concatenate((np.zeros((front_zeros, x.shape[1])), x))\n            if add_zeros and rear_zeros:\n                x = np.concatenate((x, np.zeros((rear_zeros, x.shape[1]))))\n                if rear_zeros > cfg.window_back:\n                    y = np.concatenate((y, np.zeros((rear_zeros-cfg.window_back, y.shape[1]))))\n                    \n        if verbose:\n            print('file_index =', file_index)\n            print('window_count =', self.df.loc[file_index, 'window_count'])\n            print('window_sum =', self.df.loc[file_index, 'window_sum'])\n            if file_index>0:\n                print('window-1_sum =', self.df.loc[file_index-1, 'window_sum'])\n            print('t_count =', self.df.loc[file_index, 't_count'])\n            print('window_extra =', self.df.loc[file_index, 'window_extra'])\n            print('infile_index =', infile_index)\n            print('time_index =', time_index)  \n            print('x_index =', x_index)  \n            print('front_zeros =', front_zeros)\n            print('rear_zeros =', rear_zeros)\n            \n        x = torch.tensor(x).to(torch.float32)\n        y = torch.Tensor(np.argmax(y,axis=1)).to(torch.int64)\n        return x, y\n    \n#ds_train = cFOGDataset_06(df_tdcsfog.loc[train_ids.index].reset_index(drop=True), split = 'train')\n#ds_valid = cFOGDataset_06(df_tdcsfog.loc[valid_ids.index].reset_index(drop=True), split = 'valid')\n#ds_full = cFOGDataset_06(df_tdcsfog, split = 'train')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.242132Z","iopub.execute_input":"2023-06-07T23:44:11.242596Z","iopub.status.idle":"2023-06-07T23:44:11.280333Z","shell.execute_reply.started":"2023-06-07T23:44:11.242560Z","shell.execute_reply":"2023-06-07T23:44:11.279275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def make_a_batch(ds, index=0, batch_size = 8):\n#    xbatch = np.zeros((batch_size, cfg.window_size, cfg.nfeatures))\n#    ybatch = np.zeros((batch_size, cfg.window_body))\n#    for i in range(batch_size):\n#        item_x, item_y = ds.__getitem__(index+i)\n#        xbatch[i], ybatch[i] = item_x, item_y\n#    \n#    xbatch = torch.tensor(xbatch.astype(np.float32))\n#    ybatch = torch.tensor(ybatch).to(torch.int64)\n#    return xbatch, ybatch\n\n#def make_a_batch_05(ds, index=0, batch_size = 8):\n#    xbatch = np.zeros((batch_size, cfg.window_size, cfg.nfeatures))\n#    ybatch = np.zeros((batch_size, cfg.window_body))\n#    for i in range(batch_size):\n#        item_x, item_y = ds.__getitem__(index+i)\n#        xbatch[i], ybatch[i] = item_x, item_y\n#    \n#    xbatch = torch.tensor(xbatch.astype(np.float32))\n#    ybatch = torch.tensor(ybatch).to(torch.int64)\n#    return xbatch, ybatch","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.282021Z","iopub.execute_input":"2023-06-07T23:44:11.282781Z","iopub.status.idle":"2023-06-07T23:44:11.288437Z","shell.execute_reply.started":"2023-06-07T23:44:11.282745Z","shell.execute_reply":"2023-06-07T23:44:11.287340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x01,y01gt = make_a_batch(ds_train,0)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.290221Z","iopub.execute_input":"2023-06-07T23:44:11.290938Z","iopub.status.idle":"2023-06-07T23:44:11.302307Z","shell.execute_reply.started":"2023-06-07T23:44:11.290903Z","shell.execute_reply":"2023-06-07T23:44:11.301326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropout = nn.Dropout(cfg.model_dropout)\nconv1 = nn.Conv1d(cfg.nfeatures, 16, cfg.layers[0])\nconv2 = nn.Conv1d(16, 16, cfg.layers[1])\nconv3 = nn.Conv1d(16, 16, cfg.layers[2])\nconv4 = nn.Conv1d(16, 16, cfg.layers[3])\nlinear = nn.Linear(16, cfg.nlabels+1)\nrelu = nn.ReLU()\nsoftmax = nn.Softmax(dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.304048Z","iopub.execute_input":"2023-06-07T23:44:11.304878Z","iopub.status.idle":"2023-06-07T23:44:11.338154Z","shell.execute_reply.started":"2023-06-07T23:44:11.304840Z","shell.execute_reply":"2023-06-07T23:44:11.337122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#file0 = ds_train.get_file_batch(0)\n#file0[1].shape\n#x = file0[0]\n#with torch.no_grad():\n#    y = model1.forward(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.339917Z","iopub.execute_input":"2023-06-07T23:44:11.340731Z","iopub.status.idle":"2023-06-07T23:44:11.345292Z","shell.execute_reply.started":"2023-06-07T23:44:11.340695Z","shell.execute_reply":"2023-06-07T23:44:11.344352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x = x01\n#x = x.to(torch.float)\n#print(x.shape)\n#x = x.transpose(1,2)\n#print(x.shape)\n#x = conv1(x)\n#print(x.shape)\n#x = conv2(x)\n#print(x.shape)\n#x = conv3(x)\n#print(x.shape)\n#x = conv4(x)\n#print(x.shape)\n#x = x.transpose(1,2)\n#print(x.shape)\n#x = x.view(-1,16)\n#print(x.shape)\n#x = linear(x)\n#print(x.shape)\n#x = softmax(x)\n#print(x.shape)\n#print(x[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.347108Z","iopub.execute_input":"2023-06-07T23:44:11.347869Z","iopub.status.idle":"2023-06-07T23:44:11.356567Z","shell.execute_reply.started":"2023-06-07T23:44:11.347829Z","shell.execute_reply":"2023-06-07T23:44:11.355422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FOGModel(nn.Module):\n    def __init__(self, p=cfg.model_dropout, nblocks=cfg.model_nblocks, layers_list = cfg.layers):\n        super(FOGModel, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.mid = cfg.model_middles\n        self.nlabels = cfg.nlabels\n        self.conv1 = nn.Conv1d(cfg.nfeatures, self.mid[0], layers_list[0])\n        self.conv2 = nn.Conv1d(self.mid[0], self.mid[1], layers_list[1])\n        self.conv3 = nn.Conv1d(self.mid[1], self.mid[2], layers_list[2])\n        self.conv4 = nn.Conv1d(self.mid[2], self.mid[3], layers_list[3])\n        self.relu = nn.ReLU()\n        self.linear1 = nn.Linear(self.mid[3], cfg.nlabels+1)\n        #self.linear2 = nn.Linear(self.mid[3], cfg.nlabels+1)\n        #self.softmax = nn.Softmax(dim=2)\n        #self.softmax = F.log_softmax(dim=2)\n\n        self.dropout = nn.Dropout(cfg.model_dropout)\n\n        \n        \n    def forward(self, x):\n        x = x.transpose(1,2)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.conv3(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.conv4(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = x.transpose(1,2)\n        x = self.linear1(x)\n        #x = self.relu(x)\n        \n        #x = self.linear2(x) \n        x = F.log_softmax(x, dim=2)\n        return x#[:,:,:-1]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.358422Z","iopub.execute_input":"2023-06-07T23:44:11.359298Z","iopub.status.idle":"2023-06-07T23:44:11.373153Z","shell.execute_reply.started":"2023-06-07T23:44:11.359264Z","shell.execute_reply":"2023-06-07T23:44:11.372126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock_01(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dropout_rate):\n        super(ConvBlock_01, self).__init__()\n\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size)\n        self.batch_norm = nn.BatchNorm1d(out_channels)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.skip_offset = kernel_size//2\n\n        self.skip_connection = nn.Conv1d(in_channels, out_channels, kernel_size=1) \\\n            if in_channels != out_channels else nn.Identity()\n        self.identity = nn.Identity()\n\n    def forward(self, x):\n        #x = x.transpose(1, 2)\n        skip = self.skip_connection(x)\n        #skip = self.identity(x)\n\n        x = self.conv(x)\n        #print('x:',x.shape, 'skip:', skip.shape, 'skip fixed:', skip[:,:,self.skip_offset:self.skip_offset+x.shape[2]].shape)\n        #skip = skip[:,:,self.skip_offset:self.skip_offset+x.shape[2]]\n        #print('x:',x.shape, 'skip:', skip.shape, 'skip fixed:', skip[:,:,self.skip_offset:self.skip_offset+x.shape[2]].shape)\n        #print('skip offset', self.skip_offset)     \n        x = self.batch_norm(x + skip[:,:,self.skip_offset:self.skip_offset+x.shape[2]])\n        x = F.relu(x)\n        x = self.dropout(x)\n        #x = x.transpose(1, 2)\n\n        return x\n    \nclass FOGModel_02(nn.Module):\n    def __init__(self, dropout_rate=cfg.model_dropout, in_channels=cfg.nfeatures, nblocks=cfg.model_nblocks, layers_list = cfg.layers):\n        super(FOGModel_02, self).__init__()\n        #self.dropout = nn.Dropout(p)\n        self.dropout_rate = dropout_rate\n        self.nblocks = cfg.model_nblocks\n        self.nlabels = cfg.nlabels\n        self.in_channels = in_channels\n        self.out_channels = cfg.nlabels\n        self.mid = cfg.model_middles\n        self.relu = nn.ReLU()\n        self.linear = nn.Linear(self.mid[3], self.out_channels+1)\n        #self.softmax = nn.Softmax(dim=2)\n        \n        self.blocks = nn.Sequential(*[\n            ConvBlock_01(in_channels if i == 0 else self.mid[i-1], \n                      self.mid[i], \n                      layers_list[i], \n                      self.dropout_rate)\n            for i in range(nblocks)\n        ])       \n        \n    def forward(self, x):\n        x = x.transpose(1,2)\n        x = self.blocks(x)\n        x = x.transpose(1,2)\n        x = self.linear(x)\n        x = F.log_softmax(x) \n        return x#[:,:,:-1]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.374976Z","iopub.execute_input":"2023-06-07T23:44:11.375855Z","iopub.status.idle":"2023-06-07T23:44:11.392795Z","shell.execute_reply.started":"2023-06-07T23:44:11.375818Z","shell.execute_reply":"2023-06-07T23:44:11.391614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model2 = FOGModel_02()\n#model1 = FOGModel()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.394620Z","iopub.execute_input":"2023-06-07T23:44:11.395329Z","iopub.status.idle":"2023-06-07T23:44:11.407104Z","shell.execute_reply.started":"2023-06-07T23:44:11.395293Z","shell.execute_reply":"2023-06-07T23:44:11.406027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_loader = DataLoader(ds_train, batch_size=cfg.batch_size, num_workers=cfg.num_workers, shuffle=True)\n#for batch in train_loader:\n#    xb, yb = batch\n#    break","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.408931Z","iopub.execute_input":"2023-06-07T23:44:11.409672Z","iopub.status.idle":"2023-06-07T23:44:11.417523Z","shell.execute_reply.started":"2023-06-07T23:44:11.409635Z","shell.execute_reply":"2023-06-07T23:44:11.416499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y1 = model2.forward(xb)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.419211Z","iopub.execute_input":"2023-06-07T23:44:11.419855Z","iopub.status.idle":"2023-06-07T23:44:11.431754Z","shell.execute_reply.started":"2023-06-07T23:44:11.419820Z","shell.execute_reply":"2023-06-07T23:44:11.430604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = torch.randn((10,4))\nb = torch.randn(10)\nc = a * b[..., None]\nprint(c.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.433718Z","iopub.execute_input":"2023-06-07T23:44:11.434480Z","iopub.status.idle":"2023-06-07T23:44:11.465608Z","shell.execute_reply.started":"2023-06-07T23:44:11.434443Z","shell.execute_reply":"2023-06-07T23:44:11.464565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_x(real_x, task_list):\n    zeros = torch.zeros((cfg.window_front,cfg.nfeatures), dtype=torch.float32).to(cfg.device)\n    x = zeros\n    for task in task_list:\n        x = torch.cat([real_x[ task[i][0]:task[i][1] ], zeros])\n    return x\n\ndef make_y(real_y, task_list):\n    index=0\n\ndef make_y_pred(real_y, task_list):\n    y_pred = torch.empty((0,cfg.nlabels), dtype=torch.float32).to(cfg.device)\n    for task in task_list:\n        y_pred = torch.cat( [ real_y[task[i][0]:task[i][1]] ] )\n    return y_pred\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T19:32:26.218492Z","iopub.execute_input":"2023-06-07T19:32:26.219217Z","iopub.status.idle":"2023-06-07T19:32:26.227667Z","shell.execute_reply.started":"2023-06-07T19:32:26.219183Z","shell.execute_reply":"2023-06-07T19:32:26.226700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, criterion):\n    loss_sum = 0.\n    scaler = GradScaler()\n    \n    model.train()\n    for x,y, t in loader:        \n    #for x,y,t in tqdm(loader):       \n        x = x.to(cfg.device)\n        y = y.to(cfg.device)\n        t = t.view(-1).to(cfg.device)\n        #print(x.shape, y.shape, t.shape)\n   \n        y_pred = model(x)\n        #print(x.shape, y.shape, y_pred.shape, t.shape)\n\n        loss = criterion(y_pred.view(-1, y_pred.size(-1))*t[..., None], y.view(-1)*t)           \n        \n        # loss.backward()\n        scaler.scale(loss).backward()\n        # optimizer.step()\n        scaler.step(optimizer)\n        scaler.update()       \n        optimizer.zero_grad()  \n        \n        loss_sum += loss.item()\n    \n    print(f\"Train Loss: {(loss_sum/len(loader)):.04f}\")\n    return loss_sum/len(loader)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.467392Z","iopub.execute_input":"2023-06-07T23:44:11.468035Z","iopub.status.idle":"2023-06-07T23:44:11.477211Z","shell.execute_reply.started":"2023-06-07T23:44:11.467997Z","shell.execute_reply":"2023-06-07T23:44:11.476246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_one_epoch(model, loader, criterion, metric):\n    loss_sum = 0.\n    y_true_epoch = torch.empty((0), dtype=torch.int64).to(cfg.device)\n    y_pred_epoch = torch.empty((0,cfg.nlabels), dtype=torch.float32).to(cfg.device)\n    #t_valid_epoch = []\n    \n    model.eval()\n    #for x,y in tqdm(loader):\n    for x,y,t in loader:\n        plus, minus = plus_minus_from_t(t.view(-1))\n        x = x.to(cfg.device)\n        y = y.to(cfg.device)\n        t = t.view(-1).to(cfg.device)\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred.view(-1, y_pred.size(-1))*t[..., None], y.view(-1)*t)\n        \n        loss_sum += loss.item()\n\n        y_pred = y_pred[:,:,:-1]\n        #y_true_epoch = torch.cat([y_true_epoch, y.view(-1, 1)])\n        #y_pred_epoch = torch.cat([y_pred_epoch, y_pred.view(-1, y_pred.size(-1))])\n        y = y.view(-1, 1)\n        y_pred = y_pred.view(-1, y_pred.size(-1))\n        for first, last in zip(plus, minus):\n            #print(first, last)\n            y_true_epoch = torch.cat([y_true_epoch, y[first:last]])\n            y_pred_epoch = torch.cat([y_pred_epoch, y_pred[first:last]])\n    \n    \n    #metric.update(y_pred_epoch, y_true_epoch.view(-1))\n    y_pred_2 = y_pred_epoch\n    \n    #average_precision_score()\n    #print('metric', y_pred_epoch.exp().shape, y_true_epoch.view(-1).shape)\n    if model.nlabels > 1:\n        metric.update(y_pred_epoch.exp(), y_true_epoch.view(-1))\n    else:\n        metric.update(y_pred_epoch.exp().view(-1), y_true_epoch.view(-1))\n    scores = metric.compute()\n    mean_score = scores.mean().cpu().numpy().item()\n    #print(scores)\n    #print(mean_score)\n    \n    #mask = torch.ones((1,cfg.nlabels), dtype=torch.float32).to(cfg.device)\n    #mask[0,cfg.nlabels-1] = 0\n    #metric.update(y_pred_epoch.exp()*mask, y_true_epoch.view(-1))\n    #score2 = metric.compute().mean()\n    \n    if model.nlabels == 3:\n        print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}, ClassWise: {scores[0]:.03f},{scores[1]:.03f},{scores[2]:.03f}\")    \n    elif model.nlabels == 2:\n        print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}, ClassWise: {scores[0]:.03f},{scores[1]:.03f}\")    \n    else:\n        print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}\")        \n    return mean_score, (loss_sum/len(loader))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.479042Z","iopub.execute_input":"2023-06-07T23:44:11.479825Z","iopub.status.idle":"2023-06-07T23:44:11.496935Z","shell.execute_reply.started":"2023-06-07T23:44:11.479790Z","shell.execute_reply":"2023-06-07T23:44:11.495812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_file(model, file_data):   \n    model.eval()\n    x = dataholder.make_file_batch(file_data, add_zeros=True).to(cfg.device)\n    with torch.no_grad():\n        y = model.forward(x)\n    y = np.exp(y.cpu().numpy())\n    return y","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.498784Z","iopub.execute_input":"2023-06-07T23:44:11.499723Z","iopub.status.idle":"2023-06-07T23:44:11.512358Z","shell.execute_reply.started":"2023-06-07T23:44:11.499694Z","shell.execute_reply":"2023-06-07T23:44:11.511253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_validation_graph(index, model, want_valid = True, file_id = None, want_fog = True, want_ugly = False):\n    if file_id==None:\n        x, y_true, t = dataholder.get_file_batch(index, want_valid = want_valid, want_fog = want_fog, want_ugly = want_ugly)\n    else:\n        x, y_true, t = dataholder.get_file_batch_id(file_id)\n    #get_file_batch(self, file_index, add_zeros = True, want_valid = False):\n    model.eval()\n    x = x.to(cfg.device)\n    with torch.no_grad():\n        y_pred = model.forward(x[None, :])\n    y_pred = np.exp(y_pred.cpu().numpy())\n    x = x.cpu().numpy()\n    \n    #tmpdf = pd.DataFrame(x, cfg.feature_list)\n    #tmpdf = df_tdcsfog[ df_tdcsfog['id']==file_id ]['data'].iloc[0]\n    result = pd.DataFrame( y_pred.reshape(-1, cfg.nlabels+1)[:,:cfg.nlabels], columns = cfg.label_list )\n    \n    figs, ax = plt.subplots(2,1, figsize=(18,5))\n    #tmpdf[['acc_v','acc_ml','acc_ap']].plot(ax=ax[0])\n    y_true = y_true.numpy()\n    y_true_3 = np.zeros((y_true.shape[0], 4))\n    indices = np.arange(len(y_true))\n    y_true_3[indices, y_true[indices]] = 1\n    #y_true_3 = np.zeros((y_true.shape[0], 4))\n    #y_true_3 = np.put_along_axis(y_true_3, y_true[:,None], 1, axis=1)[:,:3]\n    pd.DataFrame( y_true_3[:,:3], columns = [cfg.label_list] ).plot(ax=ax[0])\n    result.reset_index(drop=True).plot(ax=ax[1])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.514129Z","iopub.execute_input":"2023-06-07T23:44:11.514856Z","iopub.status.idle":"2023-06-07T23:44:11.527716Z","shell.execute_reply.started":"2023-06-07T23:44:11.514820Z","shell.execute_reply":"2023-06-07T23:44:11.526534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_validation_graph(3, model)\n#x, y_true = dataholder.get_file_batch(3, want_valid=True)\n#print(y_true.shape, y_true)\n#y_true = y_true.numpy()\n#y_true_3 = np.zeros((y_true.shape[0], 4))\n#indices = np.arange(len(y_true))\n#y_true_3[indices, y_true[indices]] = 1","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.530194Z","iopub.execute_input":"2023-06-07T23:44:11.530878Z","iopub.status.idle":"2023-06-07T23:44:11.541772Z","shell.execute_reply.started":"2023-06-07T23:44:11.530841Z","shell.execute_reply":"2023-06-07T23:44:11.540690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_true = np.array([3, 3, 3, 2, 1, 3, 0, 3])\n#y_true_3 = np.zeros((y_true.shape[0], 4))\n#y_true_3\n#np.put_along_axis(y_true_3, y_true[:,None], 1, axis=1)\n#y_true_3","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.543929Z","iopub.execute_input":"2023-06-07T23:44:11.545136Z","iopub.status.idle":"2023-06-07T23:44:11.553128Z","shell.execute_reply.started":"2023-06-07T23:44:11.545096Z","shell.execute_reply":"2023-06-07T23:44:11.552170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validation_one_epoch(model, valid_loader, criterion, metric)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.556098Z","iopub.execute_input":"2023-06-07T23:44:11.557512Z","iopub.status.idle":"2023-06-07T23:44:11.564290Z","shell.execute_reply.started":"2023-06-07T23:44:11.557424Z","shell.execute_reply":"2023-06-07T23:44:11.563248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n#weights = torch.tensor([0.5, 0.5, 0.5, 0.1])\n#criterion = torch.nn.CrossEntropyLoss().to(cfg.device)\n#metric = MulticlassAUPRC( num_classes=cfg.nlabels, average=None )\n#train_one_epoch(model, train_loader, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.565810Z","iopub.execute_input":"2023-06-07T23:44:11.566648Z","iopub.status.idle":"2023-06-07T23:44:11.574471Z","shell.execute_reply.started":"2023-06-07T23:44:11.566614Z","shell.execute_reply":"2023-06-07T23:44:11.573767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cfg.lr = 0.00003\n#cfg.model_dropout = 0.1\n#cfg.num_epochs = 3","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.575876Z","iopub.execute_input":"2023-06-07T23:44:11.576994Z","iopub.status.idle":"2023-06-07T23:44:11.589012Z","shell.execute_reply.started":"2023-06-07T23:44:11.576959Z","shell.execute_reply":"2023-06-07T23:44:11.587966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, optimizer, ds_train, ds_valid, num_epochs = cfg.num_epochs_per_turn, eval_freq = 1):\n    #model = FOGModel_02().to(cfg.device)\n    #model = FOGModel().to(cfg.device)\n    #print(f\"Number of parameters in model - {count_parameters(model):,}\")\n\n    #print(f\"lengths of datasets: train - {len(train_dataset)}, valid - {len(valid_dataset)}\")\n\n    train_loader = DataLoader(ds_train, batch_size=cfg.batch_size, num_workers=cfg.num_workers, shuffle=True)\n    valid_loader = DataLoader(ds_valid, batch_size=cfg.batch_size, num_workers=cfg.num_workers)\n\n    #weights = torch.tensor([0.2, 0.3, 0.1, 0.5])\n    #criterion = torch.nn.CrossEntropyLoss(weight=weights, reduction='none').to(cfg.device)\n    #criterion = torch.nn.CrossEntropyLoss().to(cfg.device)\n    criterion = torch.nn.NLLLoss().to(cfg.device)\n    if cfg.nlabels > 1:\n        metric = MulticlassAUPRC( num_classes=cfg.nlabels, average=None )\n    else:\n        metric = BinaryAUPRC()\n    # sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)\n\n    max_score, score = 0.0, 0.0\n\n    print(\"=\"*50)\n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n\n        if (epoch % eval_freq == eval_freq-1) and ( eval_freq != -1):\n            print(f\"Epoch: {epoch}\")\n            score, val_loss = validation_one_epoch(model, valid_loader, criterion, metric)\n            train_log.append([ train_loss, val_loss, score])\n            #pd.DataFrame(train_log, columns=('Epoch', 'Train loss', 'Val loss', 'Val score')).plot()\n            print(\"=\"*50)\n        # sched.step()\n\n        if score > max_score:\n            max_score = score\n            #torch.save(model.state_dict(), \"best_model_state.h5\")\n            #print(\"Saving Model ...\")\n        \n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return model, optimizer","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.590706Z","iopub.execute_input":"2023-06-07T23:44:11.591815Z","iopub.status.idle":"2023-06-07T23:44:11.604178Z","shell.execute_reply.started":"2023-06-07T23:44:11.591780Z","shell.execute_reply":"2023-06-07T23:44:11.603160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.save(model.state_dict(), \"best_model_state_01.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.605576Z","iopub.execute_input":"2023-06-07T23:44:11.607129Z","iopub.status.idle":"2023-06-07T23:44:11.620382Z","shell.execute_reply.started":"2023-06-07T23:44:11.607077Z","shell.execute_reply":"2023-06-07T23:44:11.619205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = FOGModel().to(cfg.device)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.623353Z","iopub.execute_input":"2023-06-07T23:44:11.624225Z","iopub.status.idle":"2023-06-07T23:44:11.632023Z","shell.execute_reply.started":"2023-06-07T23:44:11.624134Z","shell.execute_reply":"2023-06-07T23:44:11.631032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.lr","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.633875Z","iopub.execute_input":"2023-06-07T23:44:11.634410Z","iopub.status.idle":"2023-06-07T23:44:11.646787Z","shell.execute_reply.started":"2023-06-07T23:44:11.634337Z","shell.execute_reply":"2023-06-07T23:44:11.645555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not cfg.load_model_from_file:\n    #model = FOGModel_02().to(cfg.device)\n    model = FOGModel().to(cfg.device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n    train_log = []","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:27:38.324931Z","iopub.execute_input":"2023-06-07T23:27:38.325548Z","iopub.status.idle":"2023-06-07T23:27:38.334056Z","shell.execute_reply.started":"2023-06-07T23:27:38.325501Z","shell.execute_reply":"2023-06-07T23:27:38.333000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qq = torch.rand((1, 5861, 4))\nqq.view(-1, qq.size(-1)).shape\ntt = torch.rand((5861))\nqqqq = qq*tt[..., None]\nqqqq.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.649163Z","iopub.execute_input":"2023-06-07T23:44:11.649731Z","iopub.status.idle":"2023-06-07T23:44:11.669410Z","shell.execute_reply.started":"2023-06-07T23:44:11.649693Z","shell.execute_reply":"2023-06-07T23:44:11.668228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#model, optimizer = train_model(model, optimizer, ds_tdcs_train, ds_tdcs_valid, num_epochs = 1)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.671078Z","iopub.execute_input":"2023-06-07T23:44:11.671715Z","iopub.status.idle":"2023-06-07T23:44:11.677920Z","shell.execute_reply.started":"2023-06-07T23:44:11.671678Z","shell.execute_reply":"2023-06-07T23:44:11.676878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ds_tdcs_valid.file_ids = ds_tdcs_valid.file_ids[0:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.679632Z","iopub.execute_input":"2023-06-07T23:44:11.680372Z","iopub.status.idle":"2023-06-07T23:44:11.687869Z","shell.execute_reply.started":"2023-06-07T23:44:11.680291Z","shell.execute_reply":"2023-06-07T23:44:11.686895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = (1,1,1,1,1,1,1,1)\nplus_minus_from_t(a)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:11.689785Z","iopub.execute_input":"2023-06-07T23:44:11.690271Z","iopub.status.idle":"2023-06-07T23:44:11.701441Z","shell.execute_reply.started":"2023-06-07T23:44:11.690236Z","shell.execute_reply":"2023-06-07T23:44:11.700176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if cfg.load_model_from_file:\n    model = FOGModel()\n    model.load_state_dict(torch.load(cfg.model_file, map_location=torch.device(cfg.device)))\n    model.to(cfg.device)\n    model.eval()\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr*0.75)\n    cfg.load_model_from_file = False","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:49:20.866602Z","iopub.execute_input":"2023-06-07T23:49:20.866997Z","iopub.status.idle":"2023-06-07T23:49:20.885923Z","shell.execute_reply.started":"2023-06-07T23:49:20.866967Z","shell.execute_reply":"2023-06-07T23:49:20.884719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#num_epochs_per_turn = 20\n#num_turns = 5\n#eval_freq = 5\ntrain_log = []\n\nif not cfg.load_model_from_file:\n    for i in range(cfg.num_turns):\n        print('Big epoch', i)\n        model, optimizer = train_model(model, optimizer, ds_both_train, ds_both_valid, eval_freq = cfg.eval_freq, num_epochs = cfg.num_epochs_per_turn)\n        #model, optimizer = train_model(model, optimizer, ds_full, ds_valid, eval_freq = cfg.eval_freq, num_epochs = cfg.num_epochs_per_turn)        \n        \n    pd.DataFrame(train_log, columns=('Train loss', 'Val loss', 'Val score')).plot()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:49:22.838907Z","iopub.execute_input":"2023-06-07T23:49:22.839506Z","iopub.status.idle":"2023-06-07T23:52:08.473188Z","shell.execute_reply.started":"2023-06-07T23:49:22.839464Z","shell.execute_reply":"2023-06-07T23:52:08.472076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.load_state_dict(torch.load(\"best_model_state.h5\", map_location=torch.device(cfg.device)))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.230674Z","iopub.execute_input":"2023-06-07T23:44:15.231053Z","iopub.status.idle":"2023-06-07T23:44:15.235915Z","shell.execute_reply.started":"2023-06-07T23:44:15.231017Z","shell.execute_reply":"2023-06-07T23:44:15.234866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.array(train_log)\n#.to('cpu').numpy()\n#for log in train_log:\n#    log[3] = log[3].cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.237907Z","iopub.execute_input":"2023-06-07T23:44:15.238708Z","iopub.status.idle":"2023-06-07T23:44:15.247142Z","shell.execute_reply.started":"2023-06-07T23:44:15.238673Z","shell.execute_reply":"2023-06-07T23:44:15.245878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(train_log).plot(x='Epoch')\n#torch.save(model.state_dict(), \"model_state_02.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.248964Z","iopub.execute_input":"2023-06-07T23:44:15.249818Z","iopub.status.idle":"2023-06-07T23:44:15.259852Z","shell.execute_reply.started":"2023-06-07T23:44:15.249778Z","shell.execute_reply":"2023-06-07T23:44:15.258798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cfg.model_file\n#cfg.model_file = 'best_model_state_02.h5'","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.260986Z","iopub.execute_input":"2023-06-07T23:44:15.263830Z","iopub.status.idle":"2023-06-07T23:44:15.271048Z","shell.execute_reply.started":"2023-06-07T23:44:15.263761Z","shell.execute_reply":"2023-06-07T23:44:15.270104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if cfg.load_model_from_file:\n#    model = FOGModel()\n#    model.load_state_dict(torch.load(cfg.model_file, map_location=torch.device(cfg.device)))\n#    model.to(cfg.device)\n#    model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.272844Z","iopub.execute_input":"2023-06-07T23:44:15.273330Z","iopub.status.idle":"2023-06-07T23:44:15.282424Z","shell.execute_reply.started":"2023-06-07T23:44:15.273210Z","shell.execute_reply":"2023-06-07T23:44:15.281363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv\")\nsample_submission_defog = sample_submission","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.284465Z","iopub.execute_input":"2023-06-07T23:44:15.284849Z","iopub.status.idle":"2023-06-07T23:44:15.560529Z","shell.execute_reply.started":"2023-06-07T23:44:15.284816Z","shell.execute_reply":"2023-06-07T23:44:15.559467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_folder_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/'\ntest_files = [file for file in os.listdir(test_folder_path)]\ntest_folder_path_defog = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/'\ntest_files_defog = [file for file in os.listdir(test_folder_path_defog)]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.562245Z","iopub.execute_input":"2023-06-07T23:44:15.562606Z","iopub.status.idle":"2023-06-07T23:44:15.576210Z","shell.execute_reply.started":"2023-06-07T23:44:15.562572Z","shell.execute_reply":"2023-06-07T23:44:15.574930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_defog = pd.DataFrame()\n\nfor test_file in test_files_defog:\n    print(test_file)\n    test_df_defog = pd.read_csv(test_folder_path_defog + test_file) \n    test_df_defog.columns = [camel_to_snake(c) for c in test_df_defog.columns]\n    \n    test_df_defog[['acc_v', 'acc_ap', 'acc_ml']] = test_df_defog[['acc_v', 'acc_ap', 'acc_ml']]\n    \n    \n    test_df_new = make_features(test_df_defog, defog=True)\n    #df_tdcsfog['id'] = df_tdcsfog['id'].apply(lambda x: x.rsplit('.',1)[0])\n    test_file_id = test_file.rsplit('.',1)[0]\n    pred_new = predict_file(model, test_df_new).reshape(-1,cfg.nlabels+1)\n    #pred = np\n    pred = rescale_freq_2(pred_new, old_freq = 128, new_freq = 100)\n    id_time = [f'{test_file_id}_{t}' for t in test_df_defog['time']]\n    tmp = pd.DataFrame(pred[:,:cfg.nlabels], index=id_time)#.loc[[t != -1 for t in time]]\n    result_defog = pd.concat([result_defog, tmp])\n        \nresult_defog.columns = ['StartHesitation', 'Turn', 'Walking'][:cfg.nlabels]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:15.578397Z","iopub.execute_input":"2023-06-07T23:44:15.579217Z","iopub.status.idle":"2023-06-07T23:44:36.152579Z","shell.execute_reply.started":"2023-06-07T23:44:15.579179Z","shell.execute_reply":"2023-06-07T23:44:36.150288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame()\nfor test_file in test_files:\n    test_df = pd.read_csv(test_folder_path + test_file) \n    test_df.columns = [camel_to_snake(c) for c in test_df.columns]\n    test_df = make_features(test_df)\n    #df_tdcsfog['id'] = df_tdcsfog['id'].apply(lambda x: x.rsplit('.',1)[0])\n    test_file_id = test_file.rsplit('.',1)[0]\n    pred = predict_file(model, test_df).reshape(-1,cfg.nlabels+1)\n    #pred = np\n    id_time = [f'{test_file_id}_{t}' for t in test_df['time']]\n    tmp = pd.DataFrame(pred[:,:cfg.nlabels], index=id_time)#.loc[[t != -1 for t in time]]\n    result = pd.concat([result, tmp])\n        \nresult.columns = ['StartHesitation', 'Turn', 'Walking'][:cfg.nlabels]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:36.164136Z","iopub.execute_input":"2023-06-07T23:44:36.164497Z","iopub.status.idle":"2023-06-07T23:44:36.464167Z","shell.execute_reply.started":"2023-06-07T23:44:36.164468Z","shell.execute_reply":"2023-06-07T23:44:36.463082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = sample_submission.set_index('Id')\nsample_submission.update(result_defog)\nsample_submission.update(result)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:36.465670Z","iopub.execute_input":"2023-06-07T23:44:36.466202Z","iopub.status.idle":"2023-06-07T23:44:36.691311Z","shell.execute_reply.started":"2023-06-07T23:44:36.466162Z","shell.execute_reply":"2023-06-07T23:44:36.690118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.reset_index().to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:36.692965Z","iopub.execute_input":"2023-06-07T23:44:36.693673Z","iopub.status.idle":"2023-06-07T23:44:39.329407Z","shell.execute_reply.started":"2023-06-07T23:44:36.693627Z","shell.execute_reply":"2023-06-07T23:44:39.328109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_validation_graph(3, model)\n#print()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:39.331262Z","iopub.execute_input":"2023-06-07T23:44:39.331954Z","iopub.status.idle":"2023-06-07T23:44:40.039013Z","shell.execute_reply.started":"2023-06-07T23:44:39.331916Z","shell.execute_reply":"2023-06-07T23:44:40.038006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmpdf = plot_validation_graph(7, model)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:40.040767Z","iopub.execute_input":"2023-06-07T23:44:40.041468Z","iopub.status.idle":"2023-06-07T23:44:40.677326Z","shell.execute_reply.started":"2023-06-07T23:44:40.041432Z","shell.execute_reply":"2023-06-07T23:44:40.676048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmpdf = plot_validation_graph(19, model)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:40.678886Z","iopub.execute_input":"2023-06-07T23:44:40.679879Z","iopub.status.idle":"2023-06-07T23:44:41.355278Z","shell.execute_reply.started":"2023-06-07T23:44:40.679841Z","shell.execute_reply":"2023-06-07T23:44:41.354027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#indices = df_tdcsfog[df_tdcsfog['fog']>0].index\n#ugly_ids = valid_ids[ valid_ids.index.isin(indices)]\n#ugly_ids.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:41.356901Z","iopub.execute_input":"2023-06-07T23:44:41.357537Z","iopub.status.idle":"2023-06-07T23:44:41.362298Z","shell.execute_reply.started":"2023-06-07T23:44:41.357496Z","shell.execute_reply":"2023-06-07T23:44:41.361262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tmpdf = plot_validation_graph(20, model)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:41.363896Z","iopub.execute_input":"2023-06-07T23:44:41.364601Z","iopub.status.idle":"2023-06-07T23:44:42.014201Z","shell.execute_reply.started":"2023-06-07T23:44:41.364565Z","shell.execute_reply":"2023-06-07T23:44:42.010462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.listdir('/kaggle/working/')\n#os.listdir('/features')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:42.016237Z","iopub.execute_input":"2023-06-07T23:44:42.016669Z","iopub.status.idle":"2023-06-07T23:44:42.021697Z","shell.execute_reply.started":"2023-06-07T23:44:42.016625Z","shell.execute_reply":"2023-06-07T23:44:42.020561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figs, ax = plt.subplots(2,1, figsize=(18,5))\ntest_df[['acc_v','acc_ml','acc_ap']].plot(ax=ax[0])\nsample_submission[:len(test_df)].reset_index(drop=True).plot(ax=ax[1])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:42.023812Z","iopub.execute_input":"2023-06-07T23:44:42.024239Z","iopub.status.idle":"2023-06-07T23:44:42.678536Z","shell.execute_reply.started":"2023-06-07T23:44:42.024204Z","shell.execute_reply":"2023-06-07T23:44:42.677410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figs, ax = plt.subplots(2,1, figsize=(18,5))\ntest_df_defog[['acc_v','acc_ml','acc_ap']].plot(ax=ax[0])\nsample_submission[len(test_df):len(test_df_defog)+len(test_df)].reset_index(drop=True).plot(ax=ax[1])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:42.680305Z","iopub.execute_input":"2023-06-07T23:44:42.680940Z","iopub.status.idle":"2023-06-07T23:44:53.066824Z","shell.execute_reply.started":"2023-06-07T23:44:42.680905Z","shell.execute_reply":"2023-06-07T23:44:53.065851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_slice = x[:, cfg.window_front:cfg.window_front+cfg.window_body ,:]\n#x_slice.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:44:53.068403Z","iopub.execute_input":"2023-06-07T23:44:53.069015Z","iopub.status.idle":"2023-06-07T23:44:53.074234Z","shell.execute_reply.started":"2023-06-07T23:44:53.068981Z","shell.execute_reply":"2023-06-07T23:44:53.073127Z"},"trusted":true},"execution_count":null,"outputs":[]}]}